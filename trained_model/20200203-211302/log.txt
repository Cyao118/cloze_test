device cuda n_gpu 1 distributed training False
***** Running training *****
  Batch size = 4
  Num steps = 5550
step: 10 | train loss: 0.013060842640697956 | train acc 0.6892489194869995
step: 20 | train loss: 0.014482392929494381 | train acc 0.6729926466941833
step: 30 | train loss: 0.014300775714218616 | train acc 0.7145161032676697
step: 40 | train loss: 0.016054539009928703 | train acc 0.6993464231491089
step: 50 | train loss: 0.01764369010925293 | train acc 0.7343750596046448
step: 60 | train loss: 0.01886610873043537 | train acc 0.7936962842941284
step: 70 | train loss: 0.017888521775603294 | train acc 0.7087576985359192
step: 80 | train loss: 0.01170273032039404 | train acc 0.7348484396934509
step: 90 | train loss: 0.012313663959503174 | train acc 0.7043478488922119
step: 100 | train loss: 0.01044634822756052 | train acc 0.7507462501525879
step: 110 | train loss: 0.013373089954257011 | train acc 0.7610441446304321
step: 120 | train loss: 0.01783587969839573 | train acc 0.7845304012298584
step: 130 | train loss: 0.0162487979978323 | train acc 0.7723971009254456
step: 140 | train loss: 0.012869143858551979 | train acc 0.692307710647583
step: 150 | train loss: 0.011561746709048748 | train acc 0.7314285635948181
step: 160 | train loss: 0.00991680845618248 | train acc 0.7477611899375916
step: 170 | train loss: 0.010705008171498775 | train acc 0.7698675394058228
step: 180 | train loss: 0.013873648829758167 | train acc 0.7767441868782043
step: 190 | train loss: 0.013615558855235577 | train acc 0.8179271817207336
step: 200 | train loss: 0.013153336942195892 | train acc 0.7164179086685181
step: 210 | train loss: 0.008964168839156628 | train acc 0.7635327577590942
step: 220 | train loss: 0.009151370264589787 | train acc 0.7951806783676147
step: 230 | train loss: 0.011287652887403965 | train acc 0.7471819519996643
step: 240 | train loss: 0.013398859649896622 | train acc 0.7513416409492493
step: 250 | train loss: 0.013472609221935272 | train acc 0.8172323703765869
step: 260 | train loss: 0.014393771067261696 | train acc 0.7838982939720154
step: 270 | train loss: 0.009775659069418907 | train acc 0.7628865838050842
step: 280 | train loss: 0.009201928041875362 | train acc 0.7656903862953186
step: 290 | train loss: 0.009073710069060326 | train acc 0.7574405074119568
step: 300 | train loss: 0.011723584495484829 | train acc 0.7440272569656372
step: 310 | train loss: 0.013655457645654678 | train acc 0.7790433168411255
step: 320 | train loss: 0.016279805451631546 | train acc 0.8338461518287659
step: 330 | train loss: 0.010077424347400665 | train acc 0.770078718662262
step: 340 | train loss: 0.010551940649747849 | train acc 0.7481909990310669
step: 350 | train loss: 0.011089516803622246 | train acc 0.7398753762245178
step: 360 | train loss: 0.008962119929492474 | train acc 0.802395224571228
step: 370 | train loss: 0.01406649500131607 | train acc 0.7223427295684814
step: 380 | train loss: 0.014180226251482964 | train acc 0.841791033744812
step: 390 | train loss: 0.011197349987924099 | train acc 0.7637051343917847
step: 400 | train loss: 0.009170232340693474 | train acc 0.7755102515220642
step: 410 | train loss: 0.008657354861497879 | train acc 0.7762762904167175
step: 420 | train loss: 0.008815684355795383 | train acc 0.7893961668014526
step: 430 | train loss: 0.011023581959307194 | train acc 0.8251028656959534
step: 440 | train loss: 0.015732768923044205 | train acc 0.7999999523162842
step: 450 | train loss: 0.015818478539586067 | train acc 0.8096591234207153
step: 460 | train loss: 0.010627374053001404 | train acc 0.729106605052948
step: 470 | train loss: 0.010038860142230988 | train acc 0.7467249035835266
step: 480 | train loss: 0.009845925495028496 | train acc 0.7525925636291504
step: 490 | train loss: 0.011702151037752628 | train acc 0.7577413320541382
step: 500 | train loss: 0.012811277061700821 | train acc 0.7775423526763916
step: 510 | train loss: 0.016041280701756477 | train acc 0.8101983070373535
step: 520 | train loss: 0.0115695521235466 | train acc 0.7406779527664185
step: 530 | train loss: 0.009879997931420803 | train acc 0.7550724744796753
step: 540 | train loss: 0.009650204330682755 | train acc 0.7674772143363953
step: 550 | train loss: 0.010510941036045551 | train acc 0.7643312215805054
step: 560 | train loss: 0.01217526663094759 | train acc 0.7538167834281921
step: 570 | train loss: 0.012921049259603024 | train acc 0.8520548343658447
step: 580 | train loss: 0.012427203357219696 | train acc 0.7991169691085815
step: 590 | train loss: 0.01126961037516594 | train acc 0.7208976149559021
step: 600 | train loss: 0.008774991147220135 | train acc 0.759365975856781
step: 610 | train loss: 0.009994810447096825 | train acc 0.7699680328369141
step: 620 | train loss: 0.01198912225663662 | train acc 0.776752769947052
step: 630 | train loss: 0.014513163827359676 | train acc 0.7864583730697632
step: 640 | train loss: 0.013292454183101654 | train acc 0.8441176414489746
step: 650 | train loss: 0.00974952057003975 | train acc 0.7863924503326416
step: 660 | train loss: 0.00863395631313324 | train acc 0.7888730764389038
step: 670 | train loss: 0.009505903348326683 | train acc 0.7782874703407288
step: 680 | train loss: 0.009963338263332844 | train acc 0.7582417726516724
step: 690 | train loss: 0.014283373951911926 | train acc 0.7734553217887878
step: 700 | train loss: 0.014192774891853333 | train acc 0.8023598790168762
step: 710 | train loss: 0.009953426197171211 | train acc 0.7930367588996887
step: 720 | train loss: 0.00987807847559452 | train acc 0.760937511920929
step: 730 | train loss: 0.00952407717704773 | train acc 0.7678300142288208
step: 740 | train loss: 0.00905363168567419 | train acc 0.7899543642997742
step: 750 | train loss: 0.013465807773172855 | train acc 0.7795100212097168
step: 760 | train loss: 0.012549979612231255 | train acc 0.8502824902534485
step: 770 | train loss: 0.01402873732149601 | train acc 0.8017492890357971
step: 780 | train loss: 0.008988136425614357 | train acc 0.7610241770744324
step: 790 | train loss: 0.008969041518867016 | train acc 0.7685589790344238
step: 800 | train loss: 0.009618423879146576 | train acc 0.7650601863861084
step: 810 | train loss: 0.010347435250878334 | train acc 0.7646104097366333
step: 820 | train loss: 0.012441214174032211 | train acc 0.7972350120544434
step: 830 | train loss: 0.01504769641906023 | train acc 0.8069164156913757
step: 840 | train loss: 0.011376732960343361 | train acc 0.77183598279953
step: 850 | train loss: 0.008507968857884407 | train acc 0.7911764979362488
step: 860 | train loss: 0.008391906507313251 | train acc 0.7914201021194458
step: 870 | train loss: 0.011187891475856304 | train acc 0.7571428418159485
step: 880 | train loss: 0.012167307548224926 | train acc 0.8177340030670166
step: 890 | train loss: 0.016323216259479523 | train acc 0.8253520727157593
step: 900 | train loss: 0.01205702219158411 | train acc 0.795258641242981
step: 910 | train loss: 0.011246590875089169 | train acc 0.7338935732841492
step: 920 | train loss: 0.009034031070768833 | train acc 0.7816264629364014
step: 930 | train loss: 0.010922322049736977 | train acc 0.7566719055175781
step: 940 | train loss: 0.010103072971105576 | train acc 0.8103792667388916
step: 950 | train loss: 0.014519505202770233 | train acc 0.800000011920929
step: 960 | train loss: 0.013232596218585968 | train acc 0.8347826600074768
step: 970 | train loss: 0.009899229742586613 | train acc 0.7767295241355896
step: 980 | train loss: 0.008271620608866215 | train acc 0.7749648094177246
step: 990 | train loss: 0.008322600275278091 | train acc 0.8016806840896606
step: 1000 | train loss: 0.008960823528468609 | train acc 0.800000011920929
step: 1010 | train loss: 0.014768270775675774 | train acc 0.8080568909645081
step: 1020 | train loss: 0.013350368477404118 | train acc 0.8431952595710754
step: 1030 | train loss: 0.011755397543311119 | train acc 0.7730061411857605
step: 1040 | train loss: 0.008563576266169548 | train acc 0.7889221906661987
step: 1050 | train loss: 0.010680323466658592 | train acc 0.7388724088668823
step: 1060 | train loss: 0.008869865909218788 | train acc 0.7857142686843872
step: 1070 | train loss: 0.009708775207400322 | train acc 0.8248587250709534
step: 1080 | train loss: 0.009917287155985832 | train acc 0.8802227973937988
step: 1090 | train loss: 0.010876653715968132 | train acc 0.8419688940048218
step: 1100 | train loss: 0.009315266273915768 | train acc 0.7647929191589355
step: 1110 | train loss: 0.007702123839408159 | train acc 0.7890173196792603
step: 1120 | train loss: 0.008715501986443996 | train acc 0.7890173196792603
step: 1130 | train loss: 0.00969522725790739 | train acc 0.7720465660095215
step: 1140 | train loss: 0.013305675238370895 | train acc 0.7798165082931519
step: 1150 | train loss: 0.013230236247181892 | train acc 0.8092643022537231
step: 1160 | train loss: 0.01062050648033619 | train acc 0.7479674816131592
step: 1170 | train loss: 0.010211684741079807 | train acc 0.7492581605911255
step: 1180 | train loss: 0.009245462715625763 | train acc 0.762536883354187
step: 1190 | train loss: 0.00930829718708992 | train acc 0.7840909361839294
step: 1200 | train loss: 0.010802991688251495 | train acc 0.8023715615272522
step: 1210 | train loss: 0.01060470286756754 | train acc 0.8739726543426514
step: 1220 | train loss: 0.010179665870964527 | train acc 0.794432520866394
step: 1230 | train loss: 0.008984530344605446 | train acc 0.7947214245796204
step: 1240 | train loss: 0.010208850726485252 | train acc 0.7395683526992798
step: 1250 | train loss: 0.010669078677892685 | train acc 0.7496087551116943
step: 1260 | train loss: 0.01130386721342802 | train acc 0.7899461388587952
step: 1270 | train loss: 0.014281771145761013 | train acc 0.7983871102333069
step: 1280 | train loss: 0.014099066145718098 | train acc 0.8260869979858398
step: 1290 | train loss: 0.008530506864190102 | train acc 0.7669616341590881
step: 1300 | train loss: 0.008274519816040993 | train acc 0.7873303294181824
step: 1310 | train loss: 0.008081446401774883 | train acc 0.8027735352516174
step: 1320 | train loss: 0.009409049525856972 | train acc 0.802893340587616
step: 1330 | train loss: 0.014367209747433662 | train acc 0.7899999618530273
step: 1340 | train loss: 0.013608680106699467 | train acc 0.8459302186965942
step: 1350 | train loss: 0.009374856948852539 | train acc 0.8043088316917419
step: 1360 | train loss: 0.008491995744407177 | train acc 0.7863501310348511
step: 1370 | train loss: 0.009548970498144627 | train acc 0.8055105209350586
step: 1380 | train loss: 0.009534305892884731 | train acc 0.8507795333862305
step: 1390 | train loss: 0.008615679107606411 | train acc 0.8500000238418579
step: 1400 | train loss: 0.006702910177409649 | train acc 0.8370043635368347
step: 1410 | train loss: 0.006565566640347242 | train acc 0.8456090688705444
step: 1420 | train loss: 0.0053920806385576725 | train acc 0.880774974822998
step: 1430 | train loss: 0.007087795063853264 | train acc 0.8636363744735718
step: 1440 | train loss: 0.006907028146088123 | train acc 0.9040403962135315
step: 1450 | train loss: 0.00691057275980711 | train acc 0.9248554706573486
step: 1460 | train loss: 0.005179462023079395 | train acc 0.886996865272522
step: 1470 | train loss: 0.0069049629382789135 | train acc 0.8316546678543091
step: 1480 | train loss: 0.006275423802435398 | train acc 0.8466353416442871
step: 1490 | train loss: 0.006794061977416277 | train acc 0.8440366983413696
step: 1500 | train loss: 0.007274188566952944 | train acc 0.8720000386238098
step: 1510 | train loss: 0.007710415869951248 | train acc 0.9042552709579468
step: 1520 | train loss: 0.007781826425343752 | train acc 0.8801843523979187
step: 1530 | train loss: 0.0058383941650390625 | train acc 0.8452883362770081
step: 1540 | train loss: 0.0063781109638512135 | train acc 0.8608964085578918
step: 1550 | train loss: 0.007838997058570385 | train acc 0.831250011920929
step: 1560 | train loss: 0.009382843039929867 | train acc 0.856842041015625
step: 1570 | train loss: 0.005690482910722494 | train acc 0.9370276927947998
step: 1580 | train loss: 0.006721658632159233 | train acc 0.9382352828979492
step: 1590 | train loss: 0.007997977547347546 | train acc 0.8301886320114136
step: 1600 | train loss: 0.007106493692845106 | train acc 0.8209169507026672
step: 1610 | train loss: 0.006069250404834747 | train acc 0.8607954978942871
step: 1620 | train loss: 0.006043980363756418 | train acc 0.8744460940361023
step: 1630 | train loss: 0.007107090670615435 | train acc 0.8808081150054932
step: 1640 | train loss: 0.007502570748329163 | train acc 0.8961748480796814
step: 1650 | train loss: 0.008129987865686417 | train acc 0.8599605560302734
step: 1660 | train loss: 0.005797346588224173 | train acc 0.8544396162033081
step: 1670 | train loss: 0.005794031545519829 | train acc 0.853157103061676
step: 1680 | train loss: 0.005043281242251396 | train acc 0.8784194588661194
step: 1690 | train loss: 0.006654838565737009 | train acc 0.885185182094574
step: 1700 | train loss: 0.008739135228097439 | train acc 0.8897436261177063
step: 1710 | train loss: 0.008949347771704197 | train acc 0.9078211784362793
step: 1720 | train loss: 0.0058759343810379505 | train acc 0.8554033637046814
step: 1730 | train loss: 0.006914075929671526 | train acc 0.8267831206321716
step: 1740 | train loss: 0.007403257302939892 | train acc 0.837173581123352
step: 1750 | train loss: 0.008467355743050575 | train acc 0.8516483306884766
step: 1760 | train loss: 0.007798917591571808 | train acc 0.8962963223457336
step: 1770 | train loss: 0.0066608102060854435 | train acc 0.9239130616188049
step: 1780 | train loss: 0.00635884003713727 | train acc 0.8682842254638672
step: 1790 | train loss: 0.006795900873839855 | train acc 0.8214285373687744
step: 1800 | train loss: 0.006006603594869375 | train acc 0.8682170510292053
step: 1810 | train loss: 0.006681615486741066 | train acc 0.8535353541374207
step: 1820 | train loss: 0.006929464638233185 | train acc 0.892070472240448
step: 1830 | train loss: 0.0068092201836407185 | train acc 0.9285714030265808
step: 1840 | train loss: 0.006461998913437128 | train acc 0.8642241358757019
step: 1850 | train loss: 0.006112104281783104 | train acc 0.8476052284240723
step: 1860 | train loss: 0.006482562981545925 | train acc 0.8497024178504944
step: 1870 | train loss: 0.006685422733426094 | train acc 0.8596214652061462
step: 1880 | train loss: 0.0076028150506317616 | train acc 0.8436974883079529
step: 1890 | train loss: 0.007926540449261665 | train acc 0.8767442107200623
step: 1900 | train loss: 0.006436879746615887 | train acc 0.9497041702270508
step: 1910 | train loss: 0.005780452396720648 | train acc 0.8502907156944275
step: 1920 | train loss: 0.006874225568026304 | train acc 0.8520179390907288
step: 1930 | train loss: 0.005527777131646872 | train acc 0.8797564506530762
step: 1940 | train loss: 0.005891520995646715 | train acc 0.8611111044883728
step: 1950 | train loss: 0.006985106971114874 | train acc 0.9081885814666748
step: 1960 | train loss: 0.007570444140583277 | train acc 0.9046154022216797
step: 1970 | train loss: 0.005896191578358412 | train acc 0.871086597442627
step: 1980 | train loss: 0.006033782381564379 | train acc 0.8571428656578064
step: 1990 | train loss: 0.006789706647396088 | train acc 0.8285714387893677
step: 2000 | train loss: 0.006549354642629623 | train acc 0.8578125238418579
step: 2010 | train loss: 0.005894171539694071 | train acc 0.8793103098869324
step: 2020 | train loss: 0.006914098747074604 | train acc 0.9188405871391296
step: 2030 | train loss: 0.009371217340230942 | train acc 0.895721971988678
step: 2040 | train loss: 0.006221686489880085 | train acc 0.8512518405914307
step: 2050 | train loss: 0.006515985354781151 | train acc 0.8452200293540955
step: 2060 | train loss: 0.005357487592846155 | train acc 0.8812500238418579
step: 2070 | train loss: 0.0053739994764328 | train acc 0.8736000061035156
step: 2080 | train loss: 0.007371732499450445 | train acc 0.8918269276618958
step: 2090 | train loss: 0.00701948581263423 | train acc 0.9255952835083008
step: 2100 | train loss: 0.006820827256888151 | train acc 0.8415214419364929
step: 2110 | train loss: 0.007765800692141056 | train acc 0.8275862336158752
step: 2120 | train loss: 0.006266310345381498 | train acc 0.8547654747962952
step: 2130 | train loss: 0.0055527654476463795 | train acc 0.8813008069992065
step: 2140 | train loss: 0.007440424058586359 | train acc 0.8858351111412048
step: 2150 | train loss: 0.010257440619170666 | train acc 0.9054877758026123
step: 2160 | train loss: 0.0069737727753818035 | train acc 0.8715789318084717
step: 2170 | train loss: 0.006639162078499794 | train acc 0.8242074847221375
step: 2180 | train loss: 0.006275599356740713 | train acc 0.8447024822235107
step: 2190 | train loss: 0.005848202388733625 | train acc 0.869983971118927
step: 2200 | train loss: 0.007701798342168331 | train acc 0.857988178730011
step: 2210 | train loss: 0.00747500266879797 | train acc 0.9107142686843872
step: 2220 | train loss: 0.008375100791454315 | train acc 0.904477596282959
step: 2230 | train loss: 0.00628648279234767 | train acc 0.8421851396560669
step: 2240 | train loss: 0.006048019975423813 | train acc 0.8504273295402527
step: 2250 | train loss: 0.007109413854777813 | train acc 0.8450919985771179
step: 2260 | train loss: 0.006200670730322599 | train acc 0.8681135177612305
step: 2270 | train loss: 0.0067391712218523026 | train acc 0.8644400835037231
step: 2280 | train loss: 0.009259155951440334 | train acc 0.9142012000083923
step: 2290 | train loss: 0.006830987054854631 | train acc 0.8610525727272034
step: 2300 | train loss: 0.006277409847825766 | train acc 0.8534850478172302
step: 2310 | train loss: 0.007074448280036449 | train acc 0.8459302186965942
step: 2320 | train loss: 0.007816188037395477 | train acc 0.8578512668609619
step: 2330 | train loss: 0.006286462768912315 | train acc 0.8847517371177673
step: 2340 | train loss: 0.006219417788088322 | train acc 0.9197860956192017
step: 2350 | train loss: 0.0071311527863144875 | train acc 0.9015957117080688
step: 2360 | train loss: 0.007261062506586313 | train acc 0.8333333730697632
step: 2370 | train loss: 0.007505001034587622 | train acc 0.8481203317642212
step: 2380 | train loss: 0.006267661228775978 | train acc 0.870917558670044
step: 2390 | train loss: 0.007119097281247377 | train acc 0.8559176921844482
step: 2400 | train loss: 0.009329509921371937 | train acc 0.873350977897644
step: 2410 | train loss: 0.006959307938814163 | train acc 0.919881284236908
step: 2420 | train loss: 0.008601837791502476 | train acc 0.8271604776382446
step: 2430 | train loss: 0.005450937896966934 | train acc 0.8757225275039673
step: 2440 | train loss: 0.00582701712846756 | train acc 0.8449275493621826
step: 2450 | train loss: 0.007605728693306446 | train acc 0.8601285815238953
step: 2460 | train loss: 0.007131404243409634 | train acc 0.8763102293014526
step: 2470 | train loss: 0.00842978898435831 | train acc 0.9073569178581238
step: 2480 | train loss: 0.00642558466643095 | train acc 0.8926174640655518
step: 2490 | train loss: 0.006527237128466368 | train acc 0.8291667103767395
step: 2500 | train loss: 0.006314930971711874 | train acc 0.8554033637046814
step: 2510 | train loss: 0.005840480327606201 | train acc 0.8640483617782593
step: 2520 | train loss: 0.005947008728981018 | train acc 0.8776859641075134
step: 2530 | train loss: 0.006879306398332119 | train acc 0.8956916332244873
step: 2540 | train loss: 0.006866117473691702 | train acc 0.9127907156944275
step: 2550 | train loss: 0.005664745811372995 | train acc 0.8645357489585876
step: 2560 | train loss: 0.00622797105461359 | train acc 0.8525547385215759
step: 2570 | train loss: 0.004823340568691492 | train acc 0.8775811195373535
step: 2580 | train loss: 0.00651769433170557 | train acc 0.8564885258674622
step: 2590 | train loss: 0.008417717181146145 | train acc 0.8596882224082947
step: 2600 | train loss: 0.007981430739164352 | train acc 0.9067055583000183
step: 2610 | train loss: 0.007268976885825396 | train acc 0.8594059944152832
step: 2620 | train loss: 0.006979904603213072 | train acc 0.8443465232849121
step: 2630 | train loss: 0.006025343667715788 | train acc 0.8399412631988525
step: 2640 | train loss: 0.006014967802911997 | train acc 0.8558823466300964
step: 2650 | train loss: 0.006582345813512802 | train acc 0.8778195977210999
step: 2660 | train loss: 0.009104674682021141 | train acc 0.904347836971283
step: 2670 | train loss: 0.006874816492199898 | train acc 0.8872180581092834
step: 2680 | train loss: 0.006596243008971214 | train acc 0.8313432931900024
step: 2690 | train loss: 0.007023218087852001 | train acc 0.834319531917572
step: 2700 | train loss: 0.006800834089517593 | train acc 0.8653530478477478
step: 2710 | train loss: 0.007729148492217064 | train acc 0.8770949840545654
step: 2720 | train loss: 0.0086109209805727 | train acc 0.8872549533843994
step: 2730 | train loss: 0.006035767029970884 | train acc 0.9016393423080444
step: 2740 | train loss: 0.008107157424092293 | train acc 0.8256410360336304
step: 2750 | train loss: 0.006161055061966181 | train acc 0.8545727133750916
step: 2760 | train loss: 0.006430889014154673 | train acc 0.8514190316200256
step: 2770 | train loss: 0.007934515364468098 | train acc 0.8938271999359131
step: 2780 | train loss: 0.005781842861324549 | train acc 0.8810572624206543
step: 2790 | train loss: 0.00446271663531661 | train acc 0.882022500038147
step: 2800 | train loss: 0.00431623263284564 | train acc 0.89552241563797
step: 2810 | train loss: 0.004486503079533577 | train acc 0.9201452136039734
step: 2820 | train loss: 0.00405618641525507 | train acc 0.9392033219337463
step: 2830 | train loss: 0.004327336326241493 | train acc 0.941504180431366
step: 2840 | train loss: 0.004253912717103958 | train acc 0.9596541523933411
step: 2850 | train loss: 0.005338928196579218 | train acc 0.8930920958518982
step: 2860 | train loss: 0.00551823154091835 | train acc 0.8803544640541077
step: 2870 | train loss: 0.0037456920836120844 | train acc 0.916167676448822
step: 2880 | train loss: 0.004958584904670715 | train acc 0.9119170904159546
step: 2890 | train loss: 0.00549306208267808 | train acc 0.9179601073265076
step: 2900 | train loss: 0.004895681515336037 | train acc 0.9475219249725342
step: 2910 | train loss: 0.003304257756099105 | train acc 0.940740704536438
step: 2920 | train loss: 0.004584169015288353 | train acc 0.8814504742622375
step: 2930 | train loss: 0.00398574024438858 | train acc 0.9055472612380981
step: 2940 | train loss: 0.004768403712660074 | train acc 0.8940809965133667
step: 2950 | train loss: 0.004061473533511162 | train acc 0.9333333373069763
step: 2960 | train loss: 0.00284653645940125 | train acc 0.9694656729698181
step: 2970 | train loss: 0.0037611303851008415 | train acc 0.9647058844566345
step: 2980 | train loss: 0.004578209016472101 | train acc 0.8947368264198303
step: 2990 | train loss: 0.004573384299874306 | train acc 0.8967136144638062
step: 3000 | train loss: 0.003888465464115143 | train acc 0.9146164655685425
step: 3010 | train loss: 0.004344335291534662 | train acc 0.9247136116027832
step: 3020 | train loss: 0.0035096313804388046 | train acc 0.9625000357627869
step: 3030 | train loss: 0.0034634575713425875 | train acc 0.9714285731315613
step: 3040 | train loss: 0.004857252351939678 | train acc 0.8896797299385071
step: 3050 | train loss: 0.0047005703672766685 | train acc 0.8778409361839294
step: 3060 | train loss: 0.005788522772490978 | train acc 0.859050452709198
step: 3070 | train loss: 0.003351671854034066 | train acc 0.9292452335357666
step: 3080 | train loss: 0.004950759932398796 | train acc 0.905730128288269
step: 3090 | train loss: 0.0060435025952756405 | train acc 0.9525222778320312
step: 3100 | train loss: 0.004491396248340607 | train acc 0.915032684803009
step: 3110 | train loss: 0.0040838527493178844 | train acc 0.8911466002464294
step: 3120 | train loss: 0.004016886465251446 | train acc 0.9069767594337463
step: 3130 | train loss: 0.003543028375133872 | train acc 0.9216966032981873
step: 3140 | train loss: 0.004695951472967863 | train acc 0.9202334880828857
step: 3150 | train loss: 0.004570883233100176 | train acc 0.9550561904907227
step: 3160 | train loss: 0.002705976599827409 | train acc 0.9682996869087219
step: 3170 | train loss: 0.004117012023925781 | train acc 0.9051204323768616
step: 3180 | train loss: 0.005200345069169998 | train acc 0.8728813529014587
step: 3190 | train loss: 0.004109764005988836 | train acc 0.9216300845146179
step: 3200 | train loss: 0.0045766825787723064 | train acc 0.9014925360679626
step: 3210 | train loss: 0.004115174058824778 | train acc 0.928982675075531
step: 3220 | train loss: 0.005972863174974918 | train acc 0.9339339733123779
step: 3230 | train loss: 0.004838142544031143 | train acc 0.89453125
step: 3240 | train loss: 0.0040799640119075775 | train acc 0.9069767594337463
step: 3250 | train loss: 0.003420772962272167 | train acc 0.9120566844940186
step: 3260 | train loss: 0.0036358044017106295 | train acc 0.9110473394393921
step: 3270 | train loss: 0.004253971856087446 | train acc 0.9119266271591187
step: 3280 | train loss: 0.004320336040109396 | train acc 0.9424083828926086
step: 3290 | train loss: 0.003444102592766285 | train acc 0.9616368412971497
step: 3300 | train loss: 0.0044178105890750885 | train acc 0.898975133895874
step: 3310 | train loss: 0.003796387230977416 | train acc 0.9089595079421997
step: 3320 | train loss: 0.00520829763263464 | train acc 0.883685827255249
step: 3330 | train loss: 0.005561818368732929 | train acc 0.8987783789634705
step: 3340 | train loss: 0.005539697594940662 | train acc 0.9215686321258545
step: 3350 | train loss: 0.004154797177761793 | train acc 0.9553571343421936
step: 3360 | train loss: 0.0039030916523188353 | train acc 0.9170653820037842
step: 3370 | train loss: 0.005780202802270651 | train acc 0.8611111044883728
step: 3380 | train loss: 0.003846342209726572 | train acc 0.9039039015769958
step: 3390 | train loss: 0.00449844216927886 | train acc 0.9104991555213928
step: 3400 | train loss: 0.005203635431826115 | train acc 0.9282699823379517
step: 3410 | train loss: 0.0037532642018049955 | train acc 0.9666666984558105
step: 3420 | train loss: 0.004119652323424816 | train acc 0.9193547964096069
step: 3430 | train loss: 0.0032038195058703423 | train acc 0.9208211302757263
step: 3440 | train loss: 0.0041780476458370686 | train acc 0.8996913433074951
step: 3450 | train loss: 0.004880024120211601 | train acc 0.8961661458015442
step: 3460 | train loss: 0.0039261323399841785 | train acc 0.9269161820411682
step: 3470 | train loss: 0.006176800932735205 | train acc 0.9251337051391602
step: 3480 | train loss: 0.0028843346517533064 | train acc 0.9686609506607056
step: 3490 | train loss: 0.004210466518998146 | train acc 0.9076433181762695
step: 3500 | train loss: 0.005985419265925884 | train acc 0.8662704229354858
step: 3510 | train loss: 0.00715939886868 | train acc 0.8575757145881653
step: 3520 | train loss: 0.004043191205710173 | train acc 0.9129082560539246
step: 3530 | train loss: 0.0042973728850483894 | train acc 0.9295454025268555
step: 3540 | train loss: 0.004080058075487614 | train acc 0.9640884399414062
step: 3550 | train loss: 0.005042319186031818 | train acc 0.8990291357040405
step: 3560 | train loss: 0.003656666725873947 | train acc 0.9092159867286682
step: 3570 | train loss: 0.004093820229172707 | train acc 0.9077155590057373
step: 3580 | train loss: 0.004101969301700592 | train acc 0.9170653820037842
step: 3590 | train loss: 0.0038165447767823935 | train acc 0.9517374634742737
step: 3600 | train loss: 0.0034607411362230778 | train acc 0.9577836990356445
step: 3610 | train loss: 0.0033915371168404818 | train acc 0.9673024415969849
step: 3620 | train loss: 0.0053174495697021484 | train acc 0.8906705975532532
step: 3630 | train loss: 0.005141076166182756 | train acc 0.8805309534072876
step: 3640 | train loss: 0.00729783670976758 | train acc 0.8469539284706116
step: 3650 | train loss: 0.0037781319115310907 | train acc 0.9370229244232178
step: 3660 | train loss: 0.002920732134953141 | train acc 0.9599056839942932
step: 3670 | train loss: 0.0036855668295174837 | train acc 0.95652174949646
step: 3680 | train loss: 0.007009781897068024 | train acc 0.875432550907135
step: 3690 | train loss: 0.004163927398622036 | train acc 0.8980510234832764
step: 3700 | train loss: 0.003320713760331273 | train acc 0.9265129566192627
step: 3710 | train loss: 0.004553810227662325 | train acc 0.9111111164093018
step: 3720 | train loss: 0.004895653109997511 | train acc 0.9173076748847961
step: 3730 | train loss: 0.00278642144985497 | train acc 0.9591346383094788
step: 3740 | train loss: 0.004692734219133854 | train acc 0.9250535368919373
step: 3750 | train loss: 0.003363492898643017 | train acc 0.9248554706573486
step: 3760 | train loss: 0.0046622259542346 | train acc 0.872648298740387
step: 3770 | train loss: 0.004262885544449091 | train acc 0.9099236726760864
step: 3780 | train loss: 0.004971370100975037 | train acc 0.9077181220054626
step: 3790 | train loss: 0.005282146856188774 | train acc 0.944736897945404
step: 3800 | train loss: 0.0037490474060177803 | train acc 0.9722222685813904
step: 3810 | train loss: 0.005676847882568836 | train acc 0.8768545985221863
step: 3820 | train loss: 0.004796096123754978 | train acc 0.8948994874954224
step: 3830 | train loss: 0.004226174205541611 | train acc 0.8995502591133118
step: 3840 | train loss: 0.004041110631078482 | train acc 0.9233176708221436
step: 3850 | train loss: 0.004795527085661888 | train acc 0.9313929080963135
step: 3860 | train loss: 0.006458288989961147 | train acc 0.9352112412452698
step: 3870 | train loss: 0.004344011191278696 | train acc 0.9211618900299072
step: 3880 | train loss: 0.005448188632726669 | train acc 0.8797101974487305
step: 3890 | train loss: 0.004635991994291544 | train acc 0.8769898414611816
step: 3900 | train loss: 0.0035404344089329243 | train acc 0.9183359146118164
step: 3910 | train loss: 0.003966739401221275 | train acc 0.9337348937988281
step: 3920 | train loss: 0.004054003860801458 | train acc 0.965944230556488
step: 3930 | train loss: 0.0040171486325562 | train acc 0.9502617716789246
step: 3940 | train loss: 0.00436225114390254 | train acc 0.8809869289398193
step: 3950 | train loss: 0.004474600777029991 | train acc 0.9074354767799377
step: 3960 | train loss: 0.004357506521046162 | train acc 0.9009433388710022
step: 3970 | train loss: 0.004092972259968519 | train acc 0.9207746386528015
step: 3980 | train loss: 0.005478371400386095 | train acc 0.9223743677139282
step: 3990 | train loss: 0.0059438529424369335 | train acc 0.966292142868042
step: 4000 | train loss: 0.0045164055190980434 | train acc 0.9012945294380188
step: 4010 | train loss: 0.004081902094185352 | train acc 0.8923959732055664
step: 4020 | train loss: 0.0050488063134253025 | train acc 0.8931297659873962
step: 4030 | train loss: 0.00599623704329133 | train acc 0.8866328001022339
step: 4040 | train loss: 0.004129127599298954 | train acc 0.9429223537445068
step: 4050 | train loss: 0.0029411553405225277 | train acc 0.9617486000061035
step: 4060 | train loss: 0.0040424237959086895 | train acc 0.9376499056816101
step: 4070 | train loss: 0.004446529317647219 | train acc 0.9005848169326782
step: 4080 | train loss: 0.0031796996481716633 | train acc 0.930059552192688
step: 4090 | train loss: 0.004509601276367903 | train acc 0.9046875238418579
step: 4100 | train loss: 0.0046864901669323444 | train acc 0.9194214344024658
step: 4110 | train loss: 0.003939254209399223 | train acc 0.9466292262077332
step: 4120 | train loss: 0.002979000797495246 | train acc 0.9724770784378052
step: 4130 | train loss: 0.004560715518891811 | train acc 0.8906009793281555
step: 4140 | train loss: 0.004819398280233145 | train acc 0.8829787373542786
step: 4150 | train loss: 0.003790974384173751 | train acc 0.9220985770225525
step: 4160 | train loss: 0.002958483761176467 | train acc 0.970108687877655
step: 4170 | train loss: 0.0035419310443103313 | train acc 0.9156010150909424
step: 4180 | train loss: 0.0030276591423898935 | train acc 0.9404934644699097
step: 4190 | train loss: 0.004036403261125088 | train acc 0.913636326789856
step: 4200 | train loss: 0.002258757594972849 | train acc 0.9565891623497009
step: 4210 | train loss: 0.00330818397924304 | train acc 0.942271888256073
step: 4220 | train loss: 0.003937504719942808 | train acc 0.9449036121368408
step: 4230 | train loss: 0.0026571922935545444 | train acc 0.96683669090271
step: 4240 | train loss: 0.0034172115847468376 | train acc 0.9319526553153992
step: 4250 | train loss: 0.004116975702345371 | train acc 0.9061538577079773
step: 4260 | train loss: 0.003947676159441471 | train acc 0.9138211011886597
step: 4270 | train loss: 0.003314330242574215 | train acc 0.9385964870452881
step: 4280 | train loss: 0.004586472641676664 | train acc 0.9620253443717957
step: 4290 | train loss: 0.0026480029337108135 | train acc 0.979411780834198
step: 4300 | train loss: 0.0033846679143607616 | train acc 0.9346733689308167
step: 4310 | train loss: 0.002892889315262437 | train acc 0.9442723989486694
step: 4320 | train loss: 0.0030702599324285984 | train acc 0.9399999976158142
step: 4330 | train loss: 0.003782592248171568 | train acc 0.9109697937965393
step: 4340 | train loss: 0.003685640636831522 | train acc 0.9338374733924866
step: 4350 | train loss: 0.0025548117700964212 | train acc 0.9715909361839294
step: 4360 | train loss: 0.002439392264932394 | train acc 0.9720280170440674
step: 4370 | train loss: 0.0036247565876692533 | train acc 0.9161849617958069
step: 4380 | train loss: 0.0032302660401910543 | train acc 0.9285714626312256
step: 4390 | train loss: 0.00367815513163805 | train acc 0.9223140478134155
step: 4400 | train loss: 0.003257160307839513 | train acc 0.9630350470542908
step: 4410 | train loss: 0.002430692547932267 | train acc 0.9686684012413025
step: 4420 | train loss: 0.0021406125742942095 | train acc 0.9855907559394836
step: 4430 | train loss: 0.003357117297127843 | train acc 0.9306625723838806
step: 4440 | train loss: 0.0029160974081605673 | train acc 0.9286733269691467
step: 4450 | train loss: 0.0029920570086687803 | train acc 0.920895516872406
step: 4460 | train loss: 0.003065916709601879 | train acc 0.9256560206413269
step: 4470 | train loss: 0.004532802850008011 | train acc 0.9353448152542114
step: 4480 | train loss: 0.0025984197854995728 | train acc 0.9828080534934998
step: 4490 | train loss: 0.0030978089198470116 | train acc 0.9371534585952759
step: 4500 | train loss: 0.0041380575858056545 | train acc 0.9050279259681702
step: 4510 | train loss: 0.0038762539625167847 | train acc 0.8917378783226013
step: 4520 | train loss: 0.0032324434723705053 | train acc 0.9279140830039978
step: 4530 | train loss: 0.004023511428385973 | train acc 0.9087302088737488
step: 4540 | train loss: 0.002398161217570305 | train acc 0.9798488020896912
step: 4550 | train loss: 0.0022842620965093374 | train acc 0.9728997349739075
step: 4560 | train loss: 0.0033097725827246904 | train acc 0.9323943257331848
step: 4570 | train loss: 0.0032029403373599052 | train acc 0.9292929172515869
step: 4580 | train loss: 0.0029837065376341343 | train acc 0.9321267008781433
step: 4590 | train loss: 0.002834817161783576 | train acc 0.9465240240097046
step: 4600 | train loss: 0.0032649331260472536 | train acc 0.9546539783477783
step: 4610 | train loss: 0.0026633997913450003 | train acc 0.977337121963501
step: 4620 | train loss: 0.00534577202051878 | train acc 0.8971332311630249
step: 4630 | train loss: 0.0030321585945785046 | train acc 0.9359534382820129
step: 4640 | train loss: 0.003098141634836793 | train acc 0.9248554706573486
step: 4650 | train loss: 0.002739275572821498 | train acc 0.9527027010917664
step: 4660 | train loss: 0.002970573492348194 | train acc 0.970822274684906
step: 4670 | train loss: 0.0036055250093340874 | train acc 0.9586777091026306
step: 4680 | train loss: 0.003144226036965847 | train acc 0.9326086640357971
step: 4690 | train loss: 0.004515569657087326 | train acc 0.9071729779243469
step: 4700 | train loss: 0.003464125795289874 | train acc 0.9254385828971863
step: 4710 | train loss: 0.0032371534034609795 | train acc 0.9379968643188477
step: 4720 | train loss: 0.0031378506682813168 | train acc 0.941841721534729
step: 4730 | train loss: 0.0030200514011085033 | train acc 0.9682539701461792
step: 4740 | train loss: 0.0018376497318968177 | train acc 0.9837398529052734
step: 4750 | train loss: 0.003360643982887268 | train acc 0.936170220375061
step: 4760 | train loss: 0.004419166129082441 | train acc 0.9181286692619324
step: 4770 | train loss: 0.002514244755730033 | train acc 0.9461883306503296
step: 4780 | train loss: 0.0021916090045124292 | train acc 0.9718309640884399
step: 4790 | train loss: 0.0021884136367589235 | train acc 0.9681093692779541
step: 4800 | train loss: 0.002411704510450363 | train acc 0.9777158498764038
step: 4810 | train loss: 0.002835181076079607 | train acc 0.9403846263885498
step: 4820 | train loss: 0.0033232923597097397 | train acc 0.9165487885475159
step: 4830 | train loss: 0.003265863750129938 | train acc 0.9204052090644836
step: 4840 | train loss: 0.0035133324563503265 | train acc 0.9205397367477417
step: 4850 | train loss: 0.003193450625985861 | train acc 0.9414893388748169
step: 4860 | train loss: 0.0026861822698265314 | train acc 0.9679012894630432
step: 4870 | train loss: 0.003351766150444746 | train acc 0.9754098057746887
step: 4880 | train loss: 0.0033772792667150497 | train acc 0.9120566844940186
step: 4890 | train loss: 0.0037964587099850178 | train acc 0.8975110054016113
step: 4900 | train loss: 0.0037467426154762506 | train acc 0.9101620316505432
step: 4910 | train loss: 0.004122736398130655 | train acc 0.9332191944122314
step: 4920 | train loss: 0.0028930131811648607 | train acc 0.9649890661239624
step: 4930 | train loss: 0.002080283360555768 | train acc 0.9825073480606079
step: 4940 | train loss: 0.0029148084577172995 | train acc 0.9532374143600464
step: 4950 | train loss: 0.004069639835506678 | train acc 0.9184861779212952
step: 4960 | train loss: 0.0041235219687223434 | train acc 0.9137930870056152
step: 4970 | train loss: 0.0029502559918910265 | train acc 0.9379968643188477
step: 4980 | train loss: 0.0038469007704406977 | train acc 0.9447983503341675
step: 4990 | train loss: 0.0027648748364299536 | train acc 0.9719887971878052
step: 5000 | train loss: 0.0024159078020602465 | train acc 0.9669211506843567
step: 5010 | train loss: 0.003449783893302083 | train acc 0.9155937433242798
step: 5020 | train loss: 0.003921667579561472 | train acc 0.9191290736198425
step: 5030 | train loss: 0.0034727640450000763 | train acc 0.9189602732658386
step: 5040 | train loss: 0.0030625662766397 | train acc 0.9454854726791382
step: 5050 | train loss: 0.0028694961220026016 | train acc 0.9612403512001038
step: 5060 | train loss: 0.0026941350661218166 | train acc 0.9659091234207153
step: 5070 | train loss: 0.0032597773242741823 | train acc 0.9214175939559937
step: 5080 | train loss: 0.002793625695630908 | train acc 0.9347180724143982
step: 5090 | train loss: 0.004258603323251009 | train acc 0.914634108543396
step: 5100 | train loss: 0.004711469169706106 | train acc 0.9116278886795044
step: 5110 | train loss: 0.0025224231649190187 | train acc 0.969924807548523
step: 5120 | train loss: 0.003414248349145055 | train acc 0.9783950448036194
step: 5130 | train loss: 0.003626414341852069 | train acc 0.9417670369148254
step: 5140 | train loss: 0.0035480675287544727 | train acc 0.9177489280700684
step: 5150 | train loss: 0.0022180923260748386 | train acc 0.9518072009086609
step: 5160 | train loss: 0.004770596511662006 | train acc 0.9193798303604126
step: 5170 | train loss: 0.003045241581276059 | train acc 0.935606062412262
step: 5180 | train loss: 0.003500242019072175 | train acc 0.9641873240470886
step: 5190 | train loss: 0.0033045466989278793 | train acc 0.9615384936332703
step: 5200 | train loss: 0.0031125659588724375 | train acc 0.9147398471832275
step: 5210 | train loss: 0.0028046071529388428 | train acc 0.9253520965576172
step: 5220 | train loss: 0.0033560029696673155 | train acc 0.9305354356765747
step: 5230 | train loss: 0.003473974298685789 | train acc 0.9336650371551514
step: 5240 | train loss: 0.0027428437024354935 | train acc 0.9542742967605591
step: 5250 | train loss: 0.002880397252738476 | train acc 0.9638888835906982
step: 5260 | train loss: 0.003026343882083893 | train acc 0.9367311000823975
step: 5270 | train loss: 0.0032430568244308233 | train acc 0.9231905341148376
step: 5280 | train loss: 0.003383537521585822 | train acc 0.9267516136169434
step: 5290 | train loss: 0.0041056182235479355 | train acc 0.9125596284866333
step: 5300 | train loss: 0.003005713690072298 | train acc 0.9514170289039612
step: 5310 | train loss: 0.0021396451629698277 | train acc 0.9784946441650391
step: 5320 | train loss: 0.0029362032655626535 | train acc 0.9451219439506531
step: 5330 | train loss: 0.003431471763178706 | train acc 0.9060693383216858
step: 5340 | train loss: 0.0034546912647783756 | train acc 0.9136905074119568
step: 5350 | train loss: 0.003162720240652561 | train acc 0.9397394061088562
step: 5360 | train loss: 0.0029430254362523556 | train acc 0.9522862434387207
step: 5370 | train loss: 0.002527086064219475 | train acc 0.9693094491958618
step: 5380 | train loss: 0.0028864399064332247 | train acc 0.9686609506607056
step: 5390 | train loss: 0.003310252446681261 | train acc 0.9162929654121399
step: 5400 | train loss: 0.004762497264891863 | train acc 0.9054877758026123
step: 5410 | train loss: 0.002807753160595894 | train acc 0.944012463092804
step: 5420 | train loss: 0.0037654475308954716 | train acc 0.947826087474823
step: 5430 | train loss: 0.0029960519168525934 | train acc 0.9677419066429138
step: 5440 | train loss: 0.0028672819025814533 | train acc 0.9766082167625427
step: 5450 | train loss: 0.0026127223391085863 | train acc 0.944223165512085
step: 5460 | train loss: 0.003892837092280388 | train acc 0.9076246619224548
step: 5470 | train loss: 0.0026093379128724337 | train acc 0.943965494632721
step: 5480 | train loss: 0.002300859661772847 | train acc 0.959876537322998
step: 5490 | train loss: 0.0027191073168069124 | train acc 0.9655172228813171
step: 5500 | train loss: 0.0032671743538230658 | train acc 0.9698630571365356
step: 5510 | train loss: 0.0042629605159163475 | train acc 0.9606741666793823
step: 5520 | train loss: 0.0032194326631724834 | train acc 0.9210526347160339
step: 5530 | train loss: 0.0028895328287035227 | train acc 0.9347496032714844
step: 5540 | train loss: 0.0036532003432512283 | train acc 0.9402174353599548
step: 5550 | train loss: 0.0028137892950326204 | train acc 0.9673590660095215
***** Running evaluation *****
  Batch size = 8
***** Valid Eval results *****
  global_step = 5552
  valid_eval_accuracy = 0.8173850185235384
  valid_eval_loss = 0.6086080137128923
  valid_h_acc = 0.8024121118809341
  valid_m_acc = 0.8530400244424076
***** Running test evaluation *****
  Batch size = 8
***** Test Eval results *****
  global_step = 5552
  valid_eval_accuracy = 0.8177318513372699
  valid_eval_loss = 0.6151229448470414
  valid_h_acc = 0.8070449627314258
  valid_m_acc = 0.8455284552845529
